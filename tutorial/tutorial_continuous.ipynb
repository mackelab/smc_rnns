{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5bb93",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We will use the `pytorch` `Dataset` class for handling the data.\n",
    "\n",
    "Here we wil load some data of 20 artificial neurons that are implementing a noisy oscillator. This data is an array of dimensions `num_trials x num_units x sequence_length`, which we use to initialise a `Basic_dataset_with_trials`.\n",
    "\n",
    "If one has data without trial structure one can instead initialise the `Basic_dataset` class using an array of dimensions `num_units x sequence_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vi_rnn.datasets import Basic_dataset_with_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_all = np.load(\"tutorial_data/continuous_data.npy\")\n",
    "n_trials, dim_x, seq_len = data_all.shape\n",
    "\n",
    "# split into train and eval\n",
    "train_inds = np.full((n_trials,), False)\n",
    "train_inds[np.random.choice(np.arange(200), size=150, replace=False)] = True\n",
    "data_train = data_all[train_inds]\n",
    "data_eval = data_all[~train_inds]\n",
    "\n",
    "\n",
    "# initialise a dataset class\n",
    "task_params = {\"name\": \"tutorial_cont\"}\n",
    "dataset = Basic_dataset_with_trials(\n",
    "    task_params=task_params,\n",
    "    data=data_train,\n",
    "    data_eval=data_eval,\n",
    "    stim=None,  # you could additionally pass stimuli like this\n",
    "    stim_eval=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some example data\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 2))\n",
    "ax[0].plot(dataset.data[0].T)\n",
    "ax[1].plot(dataset.data[2].T)\n",
    "ax[0].set_xlim(0)\n",
    "ax[1].set_xlim(0)\n",
    "ax[0].set_xlabel(\"timesteps\")\n",
    "ax[1].set_xlabel(\"timesteps\")\n",
    "ax[0].set_title(\"activity in trial 0\")\n",
    "ax[1].set_title(\"activity in trial 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8b295",
   "metadata": {},
   "source": [
    "### Initialise the variational inference model\n",
    "\n",
    "All the inference functions, and the model parameters are contained in the `VAE` class, which contains \n",
    "\n",
    "\n",
    "- All the RNN parameters in: `vae.rnn`\n",
    "    - The dynamics / transition model $p(\\mathbf{z}_t\\mid\\mathbf{z}_{t-1})$ is in: `vae.rnn.transition`\n",
    "    - The observation model $p(\\mathbf{y}_t\\mid\\mathbf{z}_{t})$ is in: `vae.rnn.observation`\n",
    "- If used, the encoder parameters are in `vae.encoder`\n",
    "\n",
    "In this example. we assume linear Gaussian observations, and use the Kalman Update step to update the RNN predictions (the so-called optimal proposal), so we don't need to train an encoding network. This means we set the proposal distribution to:\n",
    "$$\n",
    "   r(\\mathbf{z}_t\\mid\\mathbf{z}_{t-1},\\mathbf{y}_{t})\\propto p(\\mathbf{y}_t\\mid\\mathbf{z}_{t})p(\\mathbf{z}_t\\mid\\mathbf{z}_{t-1})$$\n",
    "\n",
    "We will here use a one-to-one mapping between RNN units and observations. Note that generally one does not have to do this - if you want a linear map between RNN units / latents and observations set `rnn_params[\"observation\"]` to `affine`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vi_rnn.vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_params = {\n",
    "\n",
    "    # transition and observation\n",
    "    \"transition\": \"low_rank\",  # \"low_rank\" or \"full_rank\" RNN\n",
    "    \"observation\": \"one_to_one\",  # \"one_to_one\" mapping between RNN and observed units or \"affine\" mapping from the latents\n",
    "    \n",
    "    # observation settings\n",
    "    \"readout_from\": \"currents\",  # readout from the RNN activity before / after applying the non-linearty by setting this to \"currents\" / \"rates\" respectively.\n",
    "    \"train_obs_bias\": False,  # whether or not to train a bias term in the observation model\n",
    "    \"train_obs_weights\": False,  # whether or not train the weights of the observation model\n",
    "    \"obs_nonlinearity\": \"identity\",  # can be used to rectify the output (e.g., when using Poisson observations, use \"softplus\")\n",
    "    \"obs_likelihood\": \"Gauss\", # observation likelihood model (\"Gauss\" or \"Poisson\")\n",
    "    \n",
    "    # transition settings\n",
    "    \"activation\": \"relu\",  # set the nonlinearity to \"clipped_relu, \"relu\", \"tanh\" or \"identity\"\n",
    "    \"decay\": 0.9,  # initial decay constant, scalar between 0 and 1\n",
    "    \"train_neuron_bias\": True,  # train a bias term for every neuron\n",
    "    \"weight_dist\": \"uniform\",  # weight distribution (\"uniform\" or \"gauss\")\n",
    "    \"initial_state\": \"trainable\",  # initial state (\"trainable\", \"zero\", or \"bias\")\n",
    "    \"simulate_input\": False, # set to True when using time-varying inputs\n",
    "\n",
    "    # noise covariances settings\n",
    "    \"train_noise_x\": True,  # whether or not to train the observation noise scale\n",
    "    \"train_noise_z\": True,  # whether or not to train the transition noise scale\n",
    "    \"train_noise_z_t0\": True,  # whether or not to train the initial state noise scale\n",
    "    \"init_noise_x\": 0.1,  # initial scale of the observation noise\n",
    "    \"init_noise_z\": 0.1,  # initial scale of the transition noise\n",
    "    \"init_noise_z_t0\": 0.1,  # initial scale of the initial state noise\n",
    "    \"noise_x\": \"diag\",  # observation covariance type (\"diag\" or \"scalar\"), can generally be left as diagional\n",
    "    \"noise_z\": \"full\",  # transition noise covariance type (\"full\", \"diag\" or \"scalar\"), set to \"full\" when using the optimal proposal\n",
    "    \"noise_z_t0\": \"full\",  # initial state noise covariance type (\"full\", \"diag\" or \"scalar\"), set to \"full\" when using the optimal proposal\n",
    "}\n",
    "\n",
    "\n",
    "VAE_params = {\n",
    "    \"dim_x\": 20,  # observation dimension (number of units in the data)\n",
    "    \"dim_z\": 2,  # latent dimension / rank of the RNN\n",
    "    \"dim_N\": 20,  # amount of units in the RNN (can generally be different then the observation dim)\n",
    "    \"dim_u\": 0,  # input stimulus dimension\n",
    "    \"rnn_params\": rnn_params,  # parameters of the RNN\n",
    "}\n",
    "\n",
    "# initialise the VAE\n",
    "vae = VAE(VAE_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13735937",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will now fit an RNN to the data, using the `train_VAE` function. There are two inference functions that can be used during training:\n",
    "- `training_params[\"loss_f\"] = \"opt_smc\"` uses `filtering_posterior_optimal_proposal()`. This can be used with linear observations and inverts the observation model closed-form.\n",
    "- `training_params[\"loss_f\"] = \"smc\"` uses `filtering_posterior()`. This can be used for arbitrary observation models, and uses an encoder to update the proposal distribution.\n",
    "\n",
    "Here we use the first as observations are Gaussian and linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vi_rnn.saving import save_model, load_model\n",
    "from vi_rnn.train import train_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"lr\": 1e-3,  # learning rate start\n",
    "    \"lr_end\": 1e-5,  # learning rate end (with exponential decay)\n",
    "    \"n_epochs\": 2,  # number of epochs to train\n",
    "    \"grad_norm\": 0,  # gradient clipping above certain norm (if this is set to >0)\n",
    "    \"batch_size\": 16,  # batch size\n",
    "    \"cuda\": False,  # train on GPU\n",
    "    \"k\": 64,  # number of particles to use\n",
    "    \"loss_f\": \"opt_smc\",  # use regular variational SMC (\"smc\"), or use the optimal (\"opt_smc\")\n",
    "    \"resample\": \"systematic\",  # type of resampling \"systematic\", \"multinomial\" or \"none\"\n",
    "    \"run_eval\": False,  # run an evaluation setup during training (requires additional parameters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "train_VAE(\n",
    "    vae,\n",
    "    training_params,\n",
    "    dataset,\n",
    "    sync_wandb=False,\n",
    "    out_dir=\"tutorial_data\",\n",
    "    fname=\"tutorial_spikes_new\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8385da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we only trained for a couple of epochs we will load a pretrained model\n",
    "# This was trained as above, but for 500 epochs\n",
    "vae, training_params, task_params = load_model(\n",
    "    \"tutorial_data/tutorial_cont\", load_encoder=False, backward_compat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bec1c",
   "metadata": {},
   "source": [
    "### Plot the trained model's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02741980",
   "metadata": {},
   "source": [
    "To generate data from our trained model, we can use the `generate` function. We can either use a data-inferred initial state, by passing the data and the argument `initial_state=\"posterior_mean\"` or just sample from the RNNs initial state by passing `initial_state=\"prior_mean\"`.\n",
    "\n",
    "We will also predict the posterior latents and smoothed rates as used during training, using the `filtering_posterior_optimal_proposal` function.\n",
    "\n",
    "Generally we get nicer visualisation of the latent dynamics by projecting the data on an orthogonalised basis. We here use the basis spanned by the left singular vectors of the RNNs weight matrix, which we obtain using the function `get_orth_proj_latents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7121c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vi_rnn.generate import generate\n",
    "from vi_rnn.inference import filtering_posterior_optimal_proposal\n",
    "from vi_rnn.utils import get_orth_proj_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data using our fit model\n",
    "Z_gen, v, data_gen, rates_gen = generate(\n",
    "    vae, u=None, x=dataset.data_eval, initial_state=\"posterior_mean\", k=1\n",
    ")\n",
    "\n",
    "# convert to numpy\n",
    "Z_gen = Z_gen[:,:,:,0].cpu().detach().numpy()\n",
    "data_gen = data_gen[:,:,:,0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bdc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posterior latents\n",
    "_, Z_inf, _ = filtering_posterior_optimal_proposal(\n",
    "    vae, dataset.data_eval, u=dataset.stim_eval, k=24, resample=\"systematic\"\n",
    ")\n",
    "# Get corresponding observations\n",
    "data_inf, _ = vae.rnn.get_observation(Z_inf)\n",
    "\n",
    "Z_inf = Z_inf.cpu().detach().numpy()\n",
    "data_inf = data_inf.cpu().detach().numpy().mean(axis=-1) # average over particles\n",
    "# convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d89ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthogonalise the latents\n",
    "projection_matrix = get_orth_proj_latents(vae)\n",
    "Z_gen_orth = np.einsum(\"BZT,OZ->BOT\", Z_gen, projection_matrix)\n",
    "Z_inf_orth = np.einsum(\"BZTK,OZ->BOTK\", Z_inf, projection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data generated by our model next to evaluation data\n",
    "# NOTE: we do not expect a timestep wise correspondance between data and generated samples\n",
    "# as there we are plotting samples from a stochastic model. Instead we expect the distributions over samples to match.\n",
    "\n",
    "alpha_particles = .1\n",
    "\n",
    "fig, ax = plt.subplots(5, 3, figsize=(6,6))\n",
    "\n",
    "\n",
    "# data\n",
    "ax[0, 0].set_ylabel(\"Observed activity\")\n",
    "ax[0, 0].plot(dataset.data_eval[0].T)\n",
    "ax[0, 1].plot(dataset.data_eval[1].T)\n",
    "ax[0, 2].plot(dataset.data_eval[2].T)\n",
    "\n",
    "# posterior\n",
    "ax[1, 0].set_ylabel(\"Inferred activity\")\n",
    "ax[1, 0].plot(data_inf[0].T)\n",
    "ax[1, 1].plot(data_inf[1].T)\n",
    "ax[1, 2].plot(data_inf[2].T)\n",
    "\n",
    "ax[2, 0].set_ylabel(\"Inferred latents\")\n",
    "ax[2, 0].plot(Z_inf_orth[0, 0], color='C0',alpha=alpha_particles)\n",
    "ax[2, 1].plot(Z_inf_orth[1, 0], color='C0',alpha=alpha_particles)\n",
    "ax[2, 2].plot(Z_inf_orth[2, 0], color='C0',alpha=alpha_particles)\n",
    "ax[2, 0].plot(Z_inf_orth[0, 1], color='C1',alpha=alpha_particles)\n",
    "ax[2, 1].plot(Z_inf_orth[1, 1], color='C1',alpha=alpha_particles)\n",
    "ax[2, 2].plot(Z_inf_orth[2, 1], color='C1',alpha=alpha_particles)\n",
    "\n",
    "# generated\n",
    "ax[3, 0].set_ylabel(\"Generated activity\")\n",
    "ax[3, 0].plot(data_gen[0].T)\n",
    "ax[3, 1].plot(data_gen[1].T)\n",
    "ax[3, 2].plot(data_gen[2].T)\n",
    "\n",
    "ax[4, 0].set_ylabel(\"Generated latents\")\n",
    "ax[4, 0].plot(Z_gen_orth[0].T)\n",
    "ax[4, 1].plot(Z_gen_orth[1].T)\n",
    "ax[4, 2].plot(Z_gen_orth[2].T)\n",
    "ax[4, 0].set_xlabel(\"timesteps\")\n",
    "\n",
    "for i, a in enumerate(ax.flatten()):\n",
    "    a.set_xlim(0, 75)\n",
    "    if i<12:\n",
    "        a.set_xticklabels([])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smc_rnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
