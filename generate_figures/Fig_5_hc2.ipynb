{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "file_dir = os.getcwd()\n",
    "sys.path.append(file_dir + \"/../\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluation.calc_stats import calc_isi_stats, calculate_correlation\n",
    "from vi_rnn.saving import load_model\n",
    "from vi_rnn.utils import get_orth_proj_latents\n",
    "from scipy.signal import coherence, welch, resample, butter, filtfilt\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from neo.io.neuroscopeio import NeuroScopeIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## we here use data from:\n",
    "## https://crcns.org/data-sets/hc/hc-2/about-hc-2\n",
    "## Mizuseki K, Sirota A, Pastalkova E, Buzs√°ki G., Neuron. 2009 Oct 29;64(2):267-80.\n",
    "## (http://www.ncbi.nlm.nih.gov/pubmed/19874793)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# model path\n",
    "filename = \"../models/hpc2/hc2rank3\"\n",
    "\n",
    "# spiking data path\n",
    "# run the preprocessing notebooks in ../training_scripts/train_hippocampus to obtain these\n",
    "test_spikes_path = \"../data_untracked/test_hpc2.npy\"\n",
    "train_spikes_path = \"../data_untracked/train_hpc2.npy\"\n",
    "\n",
    "# lfp path\n",
    "lfp_path = \"../data_untracked/ec013527.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VAE\n",
    "vae, params, task_params, training_params = load_model(str(filename))\n",
    "rank = vae.dim_z\n",
    "vae = vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_data = np.load(test_spikes_path)\n",
    "train_data = np.load(train_spikes_path)\n",
    "\n",
    "# cutting the last bits of spike data, because these are empty anyway\n",
    "# + we want to match it with the size of the LFP data.\n",
    "test_data = test_data[:, 0:-490]\n",
    "dim_x, T = test_data.shape\n",
    "_, T_train = train_data.shape\n",
    "print(\"Data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained network\n",
    "\n",
    "# get data inferred initial state\n",
    "z_hat, _,  = vae.encoder(torch.from_numpy(test_data)[:, :1000].float().unsqueeze(0))\n",
    "z0 = z_hat[0, :, 0].unsqueeze(0)\n",
    "\n",
    "# get latent trajectory (cut first 1000 time steps)\n",
    "Z = vae.rnn.get_latent_time_series(time_steps=T, cut_off=1000, z0=z0, noise_scale=1)\n",
    "\n",
    "# get spikes\n",
    "lam = vae.rnn.get_observation(Z, noise_scale=0)[0, :, :, 0]\n",
    "spikes_pred = torch.poisson(lam).T.detach().numpy()\n",
    "\n",
    "# project latents on orthogonalised connnectivity and normalise\n",
    "projection_matrix = get_orth_proj_latents(vae)\n",
    "Z = projection_matrix @ Z[0, :, :, 0]\n",
    "prior_Z = zscore(Z.detach().numpy(), axis=1)\n",
    "\n",
    "# get posterior latents\n",
    "k = 128  # number of particles\n",
    "with torch.no_grad():\n",
    "    Qzs_filt_avg, Qzs_sm_avg, Xs_filt_avg, Xs_sm_avg = vae.predict_NLB(\n",
    "        torch.from_numpy(test_data).float().unsqueeze(0),\n",
    "        u=None,\n",
    "        k=k,\n",
    "        t_held_in=T,\n",
    "        t_forward=0,\n",
    "        marginal_smoothing=False,\n",
    "    )\n",
    "Qzs_filt_avg = Qzs_filt_avg[0].mean(axis=-1)  # average over particles\n",
    "Qzs_filt_avg = (\n",
    "    projection_matrix @ Qzs_filt_avg\n",
    ")  # project on orthogonalised connectivity\n",
    "post_Z = zscore(Qzs_filt_avg.detach().numpy(), axis=1)  # normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LFP data\n",
    "reader = NeuroScopeIO(filename=lfp_path)\n",
    "seg = reader.read_segment(lazy=False)\n",
    "t, c = np.shape(seg.analogsignals[0])\n",
    "ds = []\n",
    "fs = 100  # new sampling frequency in Hz\n",
    "for i in range(c):\n",
    "    lfp = np.array(seg.analogsignals[0][:, i])\n",
    "    resample_rate = 1250 / fs\n",
    "    n_samples = int(len(lfp) / resample_rate)\n",
    "    lfp_ds = resample(lfp, n_samples)\n",
    "    ds.append(lfp_ds)\n",
    "\n",
    "lfp = np.array(ds)[:, :, 0]\n",
    "test_lfp = lfp[:, T_train : T_train + T]\n",
    "\n",
    "# z score lfp\n",
    "test_lfp = zscore(test_lfp, axis=1)\n",
    "\n",
    "# take the mean along the channels\n",
    "test_lfp = np.mean(test_lfp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate power spectral density\n",
    "nperseg = 1024\n",
    "f_test_lfp, psd_test_lfp = welch(\n",
    "    test_lfp.reshape(-1), fs=fs, nperseg=nperseg\n",
    ")  # Adjust nperseg as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandpass filter lfp and latents\n",
    "\n",
    "# initialise a filter\n",
    "t = np.arange(0, T) / fs  # Time vector\n",
    "lowcut = 1\n",
    "highcut = 40\n",
    "low = lowcut / (fs / 2)\n",
    "high = highcut / (fs / 2)\n",
    "order = 4\n",
    "b, a = butter(order, [low, high], btype=\"band\")\n",
    "\n",
    "# Apply the bandpass filter\n",
    "lfp_filtered = filtfilt(b, a, test_lfp, axis=0)\n",
    "prior_Z_filtered = filtfilt(b, a, prior_Z, axis=1)\n",
    "post_Z_filtered = filtfilt(b, a, post_Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate power spectral density of the filtered signals\n",
    "f_pZ1, psd_pZ1 = welch(prior_Z[0], fs=fs, nperseg=nperseg)\n",
    "f_pZ2, psd_pZ2 = welch(prior_Z[1], fs=fs, nperseg=nperseg)\n",
    "f_pZ3, psd_pZ3 = welch(prior_Z[2], fs=fs, nperseg=nperseg)\n",
    "f_Z1, psd_Z1 = welch(post_Z[0], fs=fs, nperseg=nperseg)\n",
    "f_Z2, psd_Z2 = welch(post_Z[1], fs=fs, nperseg=nperseg)\n",
    "f_Z3, psd_Z3 = welch(post_Z[2], fs=fs, nperseg=nperseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = \"#7B46C1\"\n",
    "color2 = \"#A860AF\"\n",
    "color3 = \"#7C277D\"\n",
    "color4 = \"#8A44A4\"\n",
    "tg = \"teal\"\n",
    "tr = \"firebrick\"\n",
    "\n",
    "fig, ax8 = plt.subplots(1, 1, figsize=(5, 3))  \n",
    "\n",
    "# psd, shown are unfiltered signals, one could also show the filtered signals\n",
    "(line0,) = ax8.semilogy(\n",
    "    f_test_lfp, psd_test_lfp, color=\"black\", alpha=0.6, zorder=0, label=\"LFP\"\n",
    ")\n",
    "(line1,) = ax8.semilogy(\n",
    "    f_pZ1, psd_pZ1, color=color1, alpha=0.9, zorder=0, label=\"z 2\"\n",
    ")\n",
    "(line2,) = ax8.semilogy(\n",
    "    f_pZ2, psd_pZ2, color=color2, alpha=0.9, zorder=0, label=\"z 3\"\n",
    ")\n",
    "(line3,) = ax8.semilogy(\n",
    "    f_pZ3, psd_pZ3, color=color3, alpha=0.9, zorder=0, label=\"z 1\"\n",
    ")\n",
    "\n",
    "# custom legend handles. note: legends were manually adjusted using illustrator afterwards to include the LFP signal\n",
    "legend_labels = [\"$LFP$\", \"$z_1$\", \"$z_2$\", \"$z_3$\"]\n",
    "legend_colors = [\"black\", color1, color2, color3]\n",
    "\n",
    "legend = ax8.legend(\n",
    "    legend_labels,\n",
    "    handletextpad=0,\n",
    "    handlelength=0,\n",
    "    fancybox=True,\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(1,1),\n",
    ")\n",
    "for text, color in zip(legend.get_texts(), legend_colors):\n",
    "    text.set_color(color)\n",
    "\n",
    "ax8.set_xlim([1, 17])\n",
    "ax8.set_ylim([10**-3, 1])\n",
    "ax8.set_title(\"psd\")\n",
    "ax8.set_xlabel(\"frequency (hz)\")\n",
    "ax8.tick_params(axis=\"y\", which=\"both\", width=1)\n",
    "ax8.set_yticks([0.01, 0.1])\n",
    "ax8.set_yticklabels([\"0.01\", \"0.1\"])\n",
    "ax8.set_xticks([1, 8, 15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlations\n",
    "corr1, _ = pearsonr(test_lfp, post_Z[0])\n",
    "corr2, _ = pearsonr(test_lfp, post_Z[1])\n",
    "corr3, _ = pearsonr(test_lfp, post_Z[2])\n",
    "corr = [corr1, corr2, corr3]\n",
    "\n",
    "p_corr1, _ = pearsonr(test_lfp, prior_Z[0])\n",
    "p_corr2, _ = pearsonr(test_lfp, prior_Z[1])\n",
    "p_corr3, _ = pearsonr(test_lfp, prior_Z[2])\n",
    "p_corr = [p_corr1, p_corr2, p_corr3]\n",
    "\n",
    "for i in range(rank):\n",
    "    print(\"Correlation between LFP and post Z{}: {}\".format(i + 1, corr[i]))\n",
    "    print(\"Correlation between LFP and prior Z{}: {}\".format(i + 1, p_corr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence\n",
    "f_coh_pZ1, Cxy_pZ1 = coherence(test_lfp, prior_Z[0], fs=fs, nperseg=1024)\n",
    "f_coh_pZ2, Cxy_pZ2 = coherence(test_lfp, prior_Z[1], fs=fs, nperseg=1024)\n",
    "f_coh_pZ3, Cxy_pZ3 = coherence(test_lfp, prior_Z[2], fs=fs, nperseg=1024)\n",
    "\n",
    "f_coh_Z1, Cxy_Z1 = coherence(test_lfp, post_Z[0], fs=fs, nperseg=1024)\n",
    "f_coh_Z2, Cxy_Z2 = coherence(test_lfp, post_Z[1], fs=fs, nperseg=1024)\n",
    "f_coh_Z3, Cxy_Z3 = coherence(test_lfp, post_Z[2], fs=fs, nperseg=1024)\n",
    "\n",
    "# convolve with gaussian kernel\n",
    "sigma = 3\n",
    "Cxy_pZ1 = gaussian_filter1d(Cxy_pZ1, sigma=sigma)\n",
    "Cxy_Z1 = gaussian_filter1d(Cxy_Z1, sigma=sigma)\n",
    "\n",
    "Cxy_pZ2 = gaussian_filter1d(Cxy_pZ2, sigma=sigma)\n",
    "Cxy_Z2 = gaussian_filter1d(Cxy_Z2, sigma=sigma)\n",
    "\n",
    "Cxy_pZ3 = gaussian_filter1d(Cxy_pZ3, sigma=sigma)\n",
    "Cxy_Z3 = gaussian_filter1d(Cxy_Z3, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrices\n",
    "test_correlation = calculate_correlation(test_data.T)\n",
    "gen_correlation = calculate_correlation(spikes_pred)\n",
    "train_correlation = calculate_correlation(train_data.T)\n",
    "\n",
    "# Extracting upper triangle values without the diagonal\n",
    "i_upper = np.triu_indices(dim_x, k=1)\n",
    "test_corr_values = test_correlation[i_upper]\n",
    "gen_corr_values = gen_correlation[i_upper]\n",
    "train_corr_values = train_correlation[i_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ISI stats\n",
    "CVs_isi_test, Means_isi_test, Std_isi_test = calc_isi_stats(test_data.T, dt=1 / fs)\n",
    "CVs_isi_gen, Means_isi_gen, Std_isi_gen = calc_isi_stats(spikes_pred, dt=1 / fs)\n",
    "CVs_isi_train, Means_isi_train, Std_isi_train = calc_isi_stats(train_data.T, dt=1 / fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get firing rates\n",
    "fr_test = np.mean(test_data, axis=1) * fs\n",
    "fr_train = np.mean(train_data, axis=1) * fs\n",
    "fr_gen = np.mean(spikes_pred, axis=0) * fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = \"#7B46C1\"\n",
    "color2 = \"#A860AF\"\n",
    "color3 = \"#7C277D\"\n",
    "color4 = \"#8A44A4\"\n",
    "tg = \"teal\"\n",
    "tr = \"firebrick\"\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "cmap2 = plt.get_cmap(\"Dark2\")\n",
    "\n",
    "with mpl.rc_context(fname=\"matplotlibrc\"):\n",
    "\n",
    "    # Create a figure with specified size\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(6, 3))\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.6)\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    ax2 = axes[0, 2]\n",
    "    ax3 = axes[0, 1]\n",
    "    ax4 = axes[1, 0]\n",
    "    ax5 = axes[1, 1]\n",
    "    ax6 = axes[1, 2]\n",
    "    ax7 = axes[0, 3]\n",
    "    ax8 = axes[1, 3]\n",
    "\n",
    "    # extract the width gap\n",
    "    gap = ax5.get_position().x0 - ax4.get_position().x1\n",
    "\n",
    "    # manually set the position for ax8\n",
    "    pos = axes[1, 3].get_position()  # Get position of the third plot\n",
    "    ax7.set_position([pos.x0 + pos.width + gap, pos.y0, pos.width, pos.height])\n",
    "\n",
    "    # manually adjust the locations of wider plots\n",
    "    width_scaling_factor = 1.6\n",
    "    new_positions = []\n",
    "\n",
    "    for i, ax in enumerate(axes[0, :3]):\n",
    "        pos = ax.get_position()\n",
    "        if i == 0:\n",
    "            new_positions.append(\n",
    "                [pos.x0, pos.y0, pos.width * width_scaling_factor, pos.height]\n",
    "            )\n",
    "        else:\n",
    "            new_positions.append(\n",
    "                [\n",
    "                    new_positions[i - 1][0] + new_positions[i - 1][2] + gap * 4 / 2.1,\n",
    "                    pos.y0,\n",
    "                    pos.width * width_scaling_factor,\n",
    "                    pos.height,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    for i, ax in enumerate(axes[0, :3]):\n",
    "        ax.set_position(new_positions[i])\n",
    "\n",
    "    # data spikes\n",
    "    sec = 3\n",
    "    init = 3100\n",
    "    duration = sec * fs\n",
    "    ax1.imshow(\n",
    "        test_data[:, init : init + duration],\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"none\",\n",
    "        vmax=1,\n",
    "    )\n",
    "    ax1.set_xticks([0, 1 * 100, 2 * 100, 3 * 100])\n",
    "    ax1.set_xticklabels([0, 1, 2, 3])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title(\"spikes from rat hippocampus\")\n",
    "    ax1.set_ylabel(\"neuron\")\n",
    "    ax1.set_xlabel(\"time (s)\")\n",
    "\n",
    "    # generated spikes\n",
    "    ax3.imshow(\n",
    "        spikes_pred[init : init + duration, :].T,\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"none\",\n",
    "        vmax=1,\n",
    "    )\n",
    "    ax3.set_xticks([0, 1 * 100, 2 * 100, 300])\n",
    "    ax3.set_xticklabels([0, 1, 2, 3])\n",
    "    ax3.set_title(\"generated spikes\")\n",
    "    ax3.set_xlabel(\"time (s)\")\n",
    "    ax3.set_yticks([])\n",
    "\n",
    "    # latents\n",
    "    t = np.linspace(0, sec, duration)\n",
    "    ax2.plot(\n",
    "        t, prior_Z[0][init : init + duration] + 5, alpha=0.9, label=\"Z1\", color=color1\n",
    "    )\n",
    "    ax2.plot(t, prior_Z[1][init : init + duration], alpha=0.9, label=\"Z2\", color=color2)\n",
    "    ax2.plot(\n",
    "        t, prior_Z[2][init : init + duration] - 5, alpha=0.9, label=\"Z3\", color=color3\n",
    "    )\n",
    "    cc = lfp_filtered[init : init + duration] + 10\n",
    "    ax2.plot(t, cc, alpha=0.7, label=\"LFP\", color=\"black\")\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xticks([0, 1, 2, 3])\n",
    "    ax2.set_xlim(0, sec)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xlabel(\"time (s)\")\n",
    "    ax2.set_title(\"latents and LFP\")\n",
    "\n",
    "    # coefficient of variation\n",
    "    # since model and test points overlap a lot, we want to make sure they are visible\n",
    "    # apply random zorder values to each point\n",
    "\n",
    "    n_dots = len(Means_isi_test)\n",
    "    zorders = np.arange(n_dots * 2)\n",
    "    np.random.shuffle(zorders)\n",
    "    for i in range(n_dots):\n",
    "\n",
    "        ax5.scatter(\n",
    "            Means_isi_test[i],\n",
    "            Means_isi_gen[i],\n",
    "            s=10,\n",
    "            alpha=0.7,\n",
    "            color=tg,\n",
    "            zorder=zorders[i],\n",
    "        )\n",
    "        ax5.scatter(\n",
    "            Means_isi_test[i],\n",
    "            Means_isi_train[i],\n",
    "            s=10,\n",
    "            alpha=0.7,\n",
    "            color=tr,\n",
    "            label=\"train\",\n",
    "            zorder=zorders[i + n_dots],\n",
    "        )\n",
    "\n",
    "    ax5.plot([0, 20], [0, 20], color=\"gray\", linestyle=\"--\", zorder=0)\n",
    "    ax5.tick_params(axis=\"x\", which=\"both\", width=1)\n",
    "    ax5.tick_params(axis=\"y\", which=\"both\", width=1)\n",
    "    ax5.set_title(\"mean ISIs (ms)\")\n",
    "    ax5.set_xscale(\"log\")\n",
    "    ax5.set_yscale(\"log\")\n",
    "    ax5.set_xlabel(\"test\")\n",
    "    ax5.set_yticks([0.1, 1, 10])\n",
    "    ax5.set_yticklabels([\"0.1\", \"1.0\", \"10\"])\n",
    "    ax5.set_xticks([0.1, 1, 10])\n",
    "    ax5.set_xticklabels([\"0.1\", \"1.0\", \"10\"])\n",
    "\n",
    "    # mean rates\n",
    "    ax4.plot(\n",
    "        np.linspace(0, 40, 20),\n",
    "        np.linspace(0, 40, 20),\n",
    "        color=\"gray\",\n",
    "        linestyle=\"--\",\n",
    "        zorder=0,\n",
    "    )\n",
    "    n_dots = len(fr_test)\n",
    "    zorders = np.arange(n_dots * 2)\n",
    "    np.random.shuffle(zorders)\n",
    "    for i in range(n_dots):\n",
    "        ax4.scatter(fr_test[i], fr_gen[i], s=10, alpha=0.7, color=tg, zorder=zorders[i])\n",
    "        ax4.scatter(\n",
    "            fr_test[i],\n",
    "            fr_train[i],\n",
    "            s=10,\n",
    "            alpha=0.7,\n",
    "            color=tr,\n",
    "            zorder=zorders[i + n_dots],\n",
    "        )\n",
    "\n",
    "    ax4.set_xscale(\"log\")\n",
    "    ax4.set_yscale(\"log\")\n",
    "    ax4.tick_params(axis=\"x\", which=\"both\", width=1)\n",
    "    ax4.tick_params(axis=\"y\", which=\"both\", width=1)\n",
    "    ax4.set_title(\"mean rates (hz)\")\n",
    "    ax4.set_yticks([1, 10])\n",
    "    ax4.set_yticklabels([\"1.0\", \"10\"])\n",
    "    ax4.set_xticks([1, 10])\n",
    "    ax4.set_xticklabels([\"1.0\", \"10\"])\n",
    "    ax4.set_xlabel(\"test\")\n",
    "    ax4.set_ylabel(\"gen / train\")\n",
    "\n",
    "    legend_labels = [\"test/gen\", \"test/train\"]\n",
    "    legend_colors = [tg, tr]\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], color=color, lw=0, label=label)\n",
    "        for color, label in zip(legend_colors, legend_labels)\n",
    "    ]\n",
    "    legend = ax4.legend(\n",
    "        handles=legend_elements,\n",
    "        handletextpad=0,\n",
    "        handlelength=0,\n",
    "        fancybox=True,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.16, 0.45),\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    for text, color in zip(legend.get_texts(), legend_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    # pairwise correlation\n",
    "    ax6.plot([-0.2, 0.4], [-0.2, 0.4], color=\"gray\", linestyle=\"--\", zorder=0)\n",
    "    dots = len(test_corr_values)\n",
    "    zorders = np.arange(n_dots * 2)\n",
    "    np.random.shuffle(zorders)\n",
    "    for i in range(n_dots):\n",
    "        ax6.scatter(\n",
    "            test_corr_values[i],\n",
    "            gen_corr_values[i],\n",
    "            s=10,\n",
    "            alpha=0.7,\n",
    "            color=tg,\n",
    "            zorder=zorders[i],\n",
    "        )\n",
    "        ax6.scatter(\n",
    "            test_corr_values[i],\n",
    "            train_corr_values[i],\n",
    "            s=10,\n",
    "            alpha=0.7,\n",
    "            color=tr,\n",
    "            label=\"train\",\n",
    "            zorder=zorders[i + n_dots],\n",
    "        )\n",
    "\n",
    "    ax6.tick_params(axis=\"x\", which=\"both\", width=1)\n",
    "    ax6.tick_params(axis=\"y\", which=\"both\", width=1)\n",
    "    ax6.set_title(\"pairwise corr.\")\n",
    "    ax6.set_xlabel(\"test\")\n",
    "    ax6.set_yticks([-0.03, 0.07])\n",
    "    ax6.set_yticklabels([\"-0.03\", \"0.07\"])\n",
    "    ax6.set_xticks([-0.03, 0.07])\n",
    "    ax6.set_xticklabels([\"-0.03\", \"0.07\"])\n",
    "    ax6.set_xlim([-0.03, 0.07])\n",
    "    ax6.set_ylim([-0.03, 0.07])\n",
    "\n",
    "    # psd, shown are unfiltered signals, one could also show the filtered signals\n",
    "    (line0,) = ax8.semilogy(\n",
    "        f_test_lfp, psd_test_lfp, color=\"black\", alpha=0.6, zorder=0, label=\"LFP\"\n",
    "    )\n",
    "    (line1,) = ax8.semilogy(\n",
    "        f_pZ1, psd_pZ1, color=color1, alpha=0.9, zorder=0, label=\"z 2\"\n",
    "    )\n",
    "    (line2,) = ax8.semilogy(\n",
    "        f_pZ2, psd_pZ2, color=color2, alpha=0.9, zorder=0, label=\"z 3\"\n",
    "    )\n",
    "    (line3,) = ax8.semilogy(\n",
    "        f_pZ3, psd_pZ3, color=color3, alpha=0.9, zorder=0, label=\"z 1\"\n",
    "    )\n",
    "\n",
    "    # custom legend handles. note: legends were manually adjusted using illustrator afterwards to include the LFP signal\n",
    "    legend_labels = [\"$LFP$\", \"$z_1$\", \"$z_2$\", \"$z_3$\"]\n",
    "    legend_colors = [\"black\", color1, color2, color3]\n",
    "\n",
    "    legend = ax8.legend(\n",
    "        legend_labels,\n",
    "        handletextpad=0,\n",
    "        handlelength=0,\n",
    "        fancybox=True,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(2.9, 2.95),\n",
    "    )\n",
    "    for text, color in zip(legend.get_texts(), legend_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    ax8.set_xlim([1, 17])\n",
    "    ax8.set_ylim([10**-3, 1])\n",
    "    ax8.set_title(\"psd\")\n",
    "    ax8.set_xlabel(\"frequency (hz)\")\n",
    "    ax8.tick_params(axis=\"y\", which=\"both\", width=1)\n",
    "    ax8.set_yticks([0.01, 0.1])\n",
    "    ax8.set_yticklabels([\"0.01\", \"0.1\"])\n",
    "    ax8.set_xticks([1, 8, 15])\n",
    "\n",
    "    # LFP and zoomed latents\n",
    "    init = int(18.3 * 40) + 1000\n",
    "    duration = 1 * 40\n",
    "    t = np.linspace(0, 1, duration)\n",
    "\n",
    "    # note: we only showcase one prior latent and its corresponding posterior latent, because all latents are highly correlated\n",
    "    ax7.plot(f_coh_pZ2, Cxy_pZ2**2, color=cmap(13), label=\"posterior\", linewidth=1.5)\n",
    "    ax7.plot(f_coh_Z2, Cxy_Z2**2, color=cmap(15), label=\"gen. latents\", linewidth=1.5)\n",
    "    legend_labels = [\"posterior\", \"gen. latents\"]\n",
    "    legend_colors = [cmap(15), cmap(13)]\n",
    "    # Add the custom legend to the plot\n",
    "    legend = ax7.legend(\n",
    "        legend_labels,\n",
    "        handletextpad=0,\n",
    "        handlelength=0,\n",
    "        fancybox=True,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.3, 0.8),\n",
    "        fontsize=6,\n",
    "    )\n",
    "    for text, color in zip(legend.get_texts(), legend_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    ax7.set_xlabel(\"time (s)\")\n",
    "    ax7.set_xlim(1, 20)\n",
    "    ax7.set_xticks([1, 8, 15])\n",
    "    ax7.set_yticks([0.1, 0.2])\n",
    "    ax7.set_title(\"coherence w.r.t. LFP\")\n",
    "    ax7.set_xlabel(\"frequency (hz)\")\n",
    "\n",
    "    plt.gcf().set_size_inches(5.2, 3)\n",
    "\n",
    "    ax1.set_box_aspect(0.625)\n",
    "    ax2.set_box_aspect(0.625)\n",
    "    ax3.set_box_aspect(0.625)\n",
    "    ax7.set_box_aspect(1)\n",
    "    ax4.set_box_aspect(1)\n",
    "    ax5.set_box_aspect(1)\n",
    "    ax6.set_box_aspect(1)\n",
    "    ax8.set_box_aspect(1)\n",
    "\n",
    "    plt.savefig(\"../figures/hpc2_main.png\", dpi=300)\n",
    "    plt.savefig(\"../figures/hpc2_maincorfon.pdf\", dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.corrcoef(test_corr_values, gen_corr_values)[0, 1],\n",
    "    np.corrcoef(test_corr_values, train_corr_values)[0, 1],\n",
    ")\n",
    "\n",
    "print(np.corrcoef(fr_test, fr_gen)[0, 1], np.corrcoef(fr_test, fr_train)[0, 1])\n",
    "\n",
    "print(\n",
    "    np.corrcoef(Means_isi_test, Means_isi_gen)[0, 1],\n",
    "    np.corrcoef(Means_isi_test, Means_isi_train)[0, 1],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
